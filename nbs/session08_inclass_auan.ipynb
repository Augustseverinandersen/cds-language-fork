{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Language modelling with RNNs (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 16:37:16.281199: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 16:37:16.413027: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-24 16:37:16.413044: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-24 16:37:17.086356: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-24 16:37:17.086447: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-24 16:37:17.086454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # Ignore warnings from libraries. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt): # return vocab if it is not part of string.punctuation \n",
    "    # string.punctuation is a python model. ( a list of all string characters that er punctuations /%&Â¤#\";:_-.,*\")\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # Making lower case \n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # encoding utf8\n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus: # every head \n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # list of tokens \n",
    "        for i in range(1, len(token_list)): # order dem sequentialy\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words): # model initilisation \n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential() # sequential model\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, #\n",
    "                        10, \n",
    "                        input_length=input_len))\n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100)) # long short term memory\n",
    "    model.add(Dropout(0.1)) # drop out layer, during training everytime you make an iteration 10% of the weights should be removed. \n",
    "    # so every iteration is only 90 %. Making things a bit more diffiuclt for the model \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax')) # Softmax prediction.\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len): # seed_text = prompt.\n",
    "    for _ in range(next_words): # for how ever many in next_word.\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # get vocab \n",
    "        token_list = pad_sequences([token_list],  # pad it (zeros)\n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), # predict the next words with higest score.\n",
    "                                            axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # appending words together. \n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\",\"..\",\"assignment-3---rnns-for-text-generation-Augustseverinandersen\",\"data\",\"news_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to load the data one at a time and append *only* the headlines to our list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(data_dir + \"/\" + filename) # joining data_dir / filename. ( Creating dataframe)\n",
    "        all_headlines.extend(list(article_df[\"headline\"].values)) # Creating a list of only headlines. \n",
    "# If string \"article\" is in the headline, and just keep the headline column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean up a little bit and see how many data points we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"] # keep the headlines if they are not \"unknown\"\n",
    "len(all_headlines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call out ```clean_text()``` function and then inspect the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my beijing the sacred city',\n",
       " '6 million riders a day 1930s technology',\n",
       " 'seeking a crossborder conference',\n",
       " 'questions for despite the yuck factor leeches are big in russian medicine',\n",
       " 'who is a criminal',\n",
       " 'an antidote to europes populism',\n",
       " 'the cost of a speech',\n",
       " 'degradation of the language',\n",
       " 'on the power of being awful',\n",
       " 'trump garbles pitch on a revised health bill']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We're then going to tokenize our data, using the ```Tokenizer()``` class from ```TensorFlow```, about which you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "We then use the ```get_sequence_of_tokens()``` function we defined above, which turns every text into a sequence of tokens based on the vocabulary from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus) # tokenizing the text, and gives every word an index. Creating a vocab.\n",
    "total_words = len(tokenizer.word_index) + 1 # how many total words are there. The reason for + 1 is to account for  = out of vocabulary token. if the tensorflow does not know the word. <unk> unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'on': 8,\n",
       " 'is': 9,\n",
       " 'trump': 10,\n",
       " 'with': 11,\n",
       " 'new': 12,\n",
       " 'at': 13,\n",
       " 'how': 14,\n",
       " 'what': 15,\n",
       " 'you': 16,\n",
       " 'an': 17,\n",
       " 'from': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'trumps': 21,\n",
       " 'its': 22,\n",
       " 'your': 23,\n",
       " 'are': 24,\n",
       " 'be': 25,\n",
       " 'not': 26,\n",
       " 'us': 27,\n",
       " 'season': 28,\n",
       " 'that': 29,\n",
       " 'by': 30,\n",
       " 'about': 31,\n",
       " 'but': 32,\n",
       " 'can': 33,\n",
       " 'episode': 34,\n",
       " 'do': 35,\n",
       " 'this': 36,\n",
       " 'when': 37,\n",
       " 'york': 38,\n",
       " 'up': 39,\n",
       " 'over': 40,\n",
       " 'why': 41,\n",
       " 'no': 42,\n",
       " 'i': 43,\n",
       " 'out': 44,\n",
       " 'more': 45,\n",
       " 'my': 46,\n",
       " 'after': 47,\n",
       " 'will': 48,\n",
       " 'may': 49,\n",
       " 'we': 50,\n",
       " 'or': 51,\n",
       " 'war': 52,\n",
       " 'who': 53,\n",
       " 'his': 54,\n",
       " 'health': 55,\n",
       " 'teaching': 56,\n",
       " 'questions': 57,\n",
       " 'now': 58,\n",
       " 'president': 59,\n",
       " 'was': 60,\n",
       " 'one': 61,\n",
       " 'house': 62,\n",
       " 'get': 63,\n",
       " 'today': 64,\n",
       " 'have': 65,\n",
       " 'should': 66,\n",
       " 'into': 67,\n",
       " 'home': 68,\n",
       " 'all': 69,\n",
       " 'dont': 70,\n",
       " 'life': 71,\n",
       " 'our': 72,\n",
       " 'has': 73,\n",
       " 'plan': 74,\n",
       " 'good': 75,\n",
       " 'first': 76,\n",
       " 'gop': 77,\n",
       " '1': 78,\n",
       " 'says': 79,\n",
       " 'like': 80,\n",
       " 'white': 81,\n",
       " 'he': 82,\n",
       " '2': 83,\n",
       " 'too': 84,\n",
       " 'trade': 85,\n",
       " 'mr': 86,\n",
       " 'world': 87,\n",
       " 'big': 88,\n",
       " 'women': 89,\n",
       " 'love': 90,\n",
       " 'back': 91,\n",
       " '3': 92,\n",
       " 'right': 93,\n",
       " 'they': 94,\n",
       " 'north': 95,\n",
       " 'recap': 96,\n",
       " 'so': 97,\n",
       " 'their': 98,\n",
       " 'time': 99,\n",
       " 'just': 100,\n",
       " 'race': 101,\n",
       " 'her': 102,\n",
       " 'if': 103,\n",
       " 'russia': 104,\n",
       " 'black': 105,\n",
       " 'going': 106,\n",
       " 'better': 107,\n",
       " 'america': 108,\n",
       " 'go': 109,\n",
       " 'times': 110,\n",
       " 'donald': 111,\n",
       " 'china': 112,\n",
       " 'care': 113,\n",
       " 'way': 114,\n",
       " 'where': 115,\n",
       " 'help': 116,\n",
       " 'still': 117,\n",
       " 'say': 118,\n",
       " 'could': 119,\n",
       " 'city': 120,\n",
       " 'whats': 121,\n",
       " '6': 122,\n",
       " 'democrats': 123,\n",
       " 'end': 124,\n",
       " 'day': 125,\n",
       " 'power': 126,\n",
       " 'make': 127,\n",
       " 'variety': 128,\n",
       " 'activities': 129,\n",
       " 'man': 130,\n",
       " 'off': 131,\n",
       " 'korea': 132,\n",
       " 'news': 133,\n",
       " 'people': 134,\n",
       " 'rules': 135,\n",
       " 'tax': 136,\n",
       " 'some': 137,\n",
       " 'than': 138,\n",
       " 'own': 139,\n",
       " 'heart': 140,\n",
       " 'fight': 141,\n",
       " 'bill': 142,\n",
       " 'great': 143,\n",
       " '5': 144,\n",
       " 'two': 145,\n",
       " 'me': 146,\n",
       " 'work': 147,\n",
       " 'change': 148,\n",
       " 'vs': 149,\n",
       " 'political': 150,\n",
       " 'syria': 151,\n",
       " 'state': 152,\n",
       " 'bad': 153,\n",
       " 'court': 154,\n",
       " 'dead': 155,\n",
       " 'gun': 156,\n",
       " 'republicans': 157,\n",
       " 'again': 158,\n",
       " 'next': 159,\n",
       " 'years': 160,\n",
       " 'real': 161,\n",
       " 'another': 162,\n",
       " 'american': 163,\n",
       " 'does': 164,\n",
       " 'against': 165,\n",
       " 'deal': 166,\n",
       " 'past': 167,\n",
       " 'year': 168,\n",
       " 'take': 169,\n",
       " 'family': 170,\n",
       " 'food': 171,\n",
       " 'little': 172,\n",
       " 'here': 173,\n",
       " '2017': 174,\n",
       " 'would': 175,\n",
       " 'death': 176,\n",
       " 'want': 177,\n",
       " 'case': 178,\n",
       " 'takes': 179,\n",
       " 'chief': 180,\n",
       " 'obama': 181,\n",
       " 'school': 182,\n",
       " 'save': 183,\n",
       " 'old': 184,\n",
       " 'long': 185,\n",
       " 'then': 186,\n",
       " 'law': 187,\n",
       " 'money': 188,\n",
       " 'americans': 189,\n",
       " 'crisis': 190,\n",
       " 'talk': 191,\n",
       " 'picture': 192,\n",
       " 'call': 193,\n",
       " 'need': 194,\n",
       " 'student': 195,\n",
       " 'mind': 196,\n",
       " 'policy': 197,\n",
       " 'children': 198,\n",
       " 'vote': 199,\n",
       " 'behind': 200,\n",
       " 'men': 201,\n",
       " 'police': 202,\n",
       " 'battle': 203,\n",
       " 'future': 204,\n",
       " 'story': 205,\n",
       " 'facebook': 206,\n",
       " 'place': 207,\n",
       " 'under': 208,\n",
       " 'justice': 209,\n",
       " 'history': 210,\n",
       " 'before': 211,\n",
       " 'being': 212,\n",
       " 'wall': 213,\n",
       " 'see': 214,\n",
       " 'pay': 215,\n",
       " 'climate': 216,\n",
       " 'cant': 217,\n",
       " 'party': 218,\n",
       " 'politics': 219,\n",
       " 'homes': 220,\n",
       " 'young': 221,\n",
       " 'really': 222,\n",
       " 'risk': 223,\n",
       " 'down': 224,\n",
       " 'wont': 225,\n",
       " 'attack': 226,\n",
       " 'were': 227,\n",
       " 'think': 228,\n",
       " 'tariffs': 229,\n",
       " 'art': 230,\n",
       " 'students': 231,\n",
       " 'rise': 232,\n",
       " 'high': 233,\n",
       " 'college': 234,\n",
       " 'south': 235,\n",
       " 'eat': 236,\n",
       " 'kids': 237,\n",
       " 'immigration': 238,\n",
       " 'win': 239,\n",
       " 'keep': 240,\n",
       " 'sex': 241,\n",
       " 'leader': 242,\n",
       " 'fear': 243,\n",
       " 'republican': 244,\n",
       " 'there': 245,\n",
       " 'states': 246,\n",
       " '7': 247,\n",
       " '2018': 248,\n",
       " 'march': 249,\n",
       " 'russian': 250,\n",
       " 'without': 251,\n",
       " 'much': 252,\n",
       " 'best': 253,\n",
       " 'parents': 254,\n",
       " 'left': 255,\n",
       " 'them': 256,\n",
       " 'child': 257,\n",
       " 'california': 258,\n",
       " 'comey': 259,\n",
       " 'taking': 260,\n",
       " 'security': 261,\n",
       " 'free': 262,\n",
       " 'ban': 263,\n",
       " 'age': 264,\n",
       " 'living': 265,\n",
       " 'fix': 266,\n",
       " 'judge': 267,\n",
       " 'obamacare': 268,\n",
       " 'most': 269,\n",
       " 'while': 270,\n",
       " 'book': 271,\n",
       " 'making': 272,\n",
       " 'office': 273,\n",
       " 'border': 274,\n",
       " 'test': 275,\n",
       " 'schools': 276,\n",
       " 'subway': 277,\n",
       " 'live': 278,\n",
       " 'secret': 279,\n",
       " 'other': 280,\n",
       " 'top': 281,\n",
       " 'isnt': 282,\n",
       " 'last': 283,\n",
       " 'did': 284,\n",
       " 'french': 285,\n",
       " 'play': 286,\n",
       " 'only': 287,\n",
       " 'problem': 288,\n",
       " 'cancer': 289,\n",
       " 'open': 290,\n",
       " 'start': 291,\n",
       " 'game': 292,\n",
       " 'him': 293,\n",
       " 'fire': 294,\n",
       " 'words': 295,\n",
       " 'brooklyn': 296,\n",
       " 'nuclear': 297,\n",
       " 'control': 298,\n",
       " 'vietnam': 299,\n",
       " 'lead': 300,\n",
       " 'night': 301,\n",
       " 'close': 302,\n",
       " 'let': 303,\n",
       " 'use': 304,\n",
       " 'friday': 305,\n",
       " 'star': 306,\n",
       " 'rights': 307,\n",
       " '8': 308,\n",
       " 'inquiry': 309,\n",
       " 'know': 310,\n",
       " 'find': 311,\n",
       " 'metoo': 312,\n",
       " 'even': 313,\n",
       " '4': 314,\n",
       " 'mindful': 315,\n",
       " 'debate': 316,\n",
       " 'election': 317,\n",
       " 'many': 318,\n",
       " 'presidents': 319,\n",
       " 'senate': 320,\n",
       " 'act': 321,\n",
       " 'lost': 322,\n",
       " 'social': 323,\n",
       " 'teenagers': 324,\n",
       " 'fbi': 325,\n",
       " 'congress': 326,\n",
       " 'dies': 327,\n",
       " 'gets': 328,\n",
       " 'budget': 329,\n",
       " 'these': 330,\n",
       " 'million': 331,\n",
       " 'faces': 332,\n",
       " 'billions': 333,\n",
       " 'less': 334,\n",
       " 'missing': 335,\n",
       " 'ask': 336,\n",
       " 'face': 337,\n",
       " 'washington': 338,\n",
       " 'stand': 339,\n",
       " 'said': 340,\n",
       " 'acrostic': 341,\n",
       " 'teachers': 342,\n",
       " 'three': 343,\n",
       " 'mueller': 344,\n",
       " 'stop': 345,\n",
       " 'epa': 346,\n",
       " 'britain': 347,\n",
       " 'job': 348,\n",
       " 'getting': 349,\n",
       " 'media': 350,\n",
       " 'street': 351,\n",
       " 'door': 352,\n",
       " 'woman': 353,\n",
       " 'tale': 354,\n",
       " 'travel': 355,\n",
       " 'puzzle': 356,\n",
       " 'chinese': 357,\n",
       " 'hope': 358,\n",
       " 'lesson': 359,\n",
       " 'side': 360,\n",
       " 'era': 361,\n",
       " 'democracy': 362,\n",
       " 'global': 363,\n",
       " 'military': 364,\n",
       " 'voters': 365,\n",
       " 'truth': 366,\n",
       " 'wants': 367,\n",
       " 'talks': 368,\n",
       " 'yes': 369,\n",
       " 'supreme': 370,\n",
       " 'risks': 371,\n",
       " 'come': 372,\n",
       " 'looking': 373,\n",
       " 'im': 374,\n",
       " 'wrong': 375,\n",
       " 'mailbag': 376,\n",
       " 'market': 377,\n",
       " 'far': 378,\n",
       " 'data': 379,\n",
       " 'look': 380,\n",
       " 'plans': 381,\n",
       " 'walking': 382,\n",
       " 'favorite': 383,\n",
       " 'cuts': 384,\n",
       " 'role': 385,\n",
       " 'king': 386,\n",
       " 'things': 387,\n",
       " 'ready': 388,\n",
       " 'presidency': 389,\n",
       " 'safety': 390,\n",
       " 'tell': 391,\n",
       " 'action': 392,\n",
       " 'country': 393,\n",
       " '10': 394,\n",
       " 'needs': 395,\n",
       " 'west': 396,\n",
       " 'second': 397,\n",
       " 'moment': 398,\n",
       " 'youre': 399,\n",
       " 'made': 400,\n",
       " 'jobs': 401,\n",
       " 'finding': 402,\n",
       " 'move': 403,\n",
       " 'turn': 404,\n",
       " 'break': 405,\n",
       " 'isis': 406,\n",
       " 'building': 407,\n",
       " 'trial': 408,\n",
       " 'former': 409,\n",
       " 'across': 410,\n",
       " 'fake': 411,\n",
       " 'cut': 412,\n",
       " 'dream': 413,\n",
       " 'days': 414,\n",
       " 'americas': 415,\n",
       " 'science': 416,\n",
       " 'shutdown': 417,\n",
       " 'putin': 418,\n",
       " 'team': 419,\n",
       " 'near': 420,\n",
       " 'france': 421,\n",
       " 'de': 422,\n",
       " 'learning': 423,\n",
       " 'early': 424,\n",
       " 'aid': 425,\n",
       " 'cuomo': 426,\n",
       " 'fast': 427,\n",
       " 'try': 428,\n",
       " 'air': 429,\n",
       " 'abuse': 430,\n",
       " 'stars': 431,\n",
       " 'dept': 432,\n",
       " 'leaders': 433,\n",
       " 'found': 434,\n",
       " 'friends': 435,\n",
       " 'put': 436,\n",
       " 'coming': 437,\n",
       " 'goes': 438,\n",
       " 'voice': 439,\n",
       " 'readers': 440,\n",
       " 'hate': 441,\n",
       " 'learn': 442,\n",
       " 'red': 443,\n",
       " 'shows': 444,\n",
       " 'winter': 445,\n",
       " 'path': 446,\n",
       " 'doctor': 447,\n",
       " 'girls': 448,\n",
       " 'must': 449,\n",
       " 'drug': 450,\n",
       " 'national': 451,\n",
       " 'challenge': 452,\n",
       " 'pain': 453,\n",
       " 'hard': 454,\n",
       " 'tech': 455,\n",
       " 'feel': 456,\n",
       " 'blood': 457,\n",
       " 'got': 458,\n",
       " 'united': 459,\n",
       " 'run': 460,\n",
       " 'sale': 461,\n",
       " 'price': 462,\n",
       " 'slow': 463,\n",
       " 'john': 464,\n",
       " 'sessions': 465,\n",
       " 'pick': 466,\n",
       " 'set': 467,\n",
       " 'makes': 468,\n",
       " 'give': 469,\n",
       " 'week': 470,\n",
       " 'meet': 471,\n",
       " 'education': 472,\n",
       " 'threat': 473,\n",
       " 'bannon': 474,\n",
       " 'speech': 475,\n",
       " 'claims': 476,\n",
       " 'james': 477,\n",
       " 'crossword': 478,\n",
       " 'search': 479,\n",
       " 'heres': 480,\n",
       " 'show': 481,\n",
       " 'support': 482,\n",
       " 'meeting': 483,\n",
       " 'business': 484,\n",
       " 'sports': 485,\n",
       " 'stephen': 486,\n",
       " 'tells': 487,\n",
       " 'had': 488,\n",
       " 'well': 489,\n",
       " 'town': 490,\n",
       " 'force': 491,\n",
       " 'thats': 492,\n",
       " 'inside': 493,\n",
       " 'question': 494,\n",
       " 'legal': 495,\n",
       " 'room': 496,\n",
       " 'comes': 497,\n",
       " 'body': 498,\n",
       " 'lives': 499,\n",
       " 'looks': 500,\n",
       " 'officials': 501,\n",
       " 'becomes': 502,\n",
       " 'trip': 503,\n",
       " 'loss': 504,\n",
       " 'pressure': 505,\n",
       " 'fighting': 506,\n",
       " 'florida': 507,\n",
       " 'silence': 508,\n",
       " 'through': 509,\n",
       " 'rising': 510,\n",
       " 'memory': 511,\n",
       " 'she': 512,\n",
       " 'away': 513,\n",
       " 'trust': 514,\n",
       " 'idea': 515,\n",
       " 'small': 516,\n",
       " 'guns': 517,\n",
       " 'leave': 518,\n",
       " 'allies': 519,\n",
       " 'been': 520,\n",
       " 'doesnt': 521,\n",
       " 'might': 522,\n",
       " 'choice': 523,\n",
       " 'attacks': 524,\n",
       " 'island': 525,\n",
       " 'doctors': 526,\n",
       " 'tv': 527,\n",
       " 'reality': 528,\n",
       " 'matter': 529,\n",
       " 'met': 530,\n",
       " 'la': 531,\n",
       " 'fall': 532,\n",
       " 'modern': 533,\n",
       " 'road': 534,\n",
       " 'hes': 535,\n",
       " 'syrian': 536,\n",
       " 'meets': 537,\n",
       " 'music': 538,\n",
       " 'didnt': 539,\n",
       " 'baby': 540,\n",
       " 'tied': 541,\n",
       " 'raise': 542,\n",
       " 'tests': 543,\n",
       " 'israel': 544,\n",
       " 'hot': 545,\n",
       " 'workers': 546,\n",
       " 'jimmy': 547,\n",
       " 'governor': 548,\n",
       " 'ways': 549,\n",
       " 'report': 550,\n",
       " 'wins': 551,\n",
       " 'privacy': 552,\n",
       " 'economy': 553,\n",
       " 'advice': 554,\n",
       " 'feud': 555,\n",
       " 'land': 556,\n",
       " 'video': 557,\n",
       " 'crash': 558,\n",
       " 'brain': 559,\n",
       " 'view': 560,\n",
       " 'others': 561,\n",
       " 'texas': 562,\n",
       " 'something': 563,\n",
       " 'worth': 564,\n",
       " 'bar': 565,\n",
       " 'word': 566,\n",
       " 'once': 567,\n",
       " 'europe': 568,\n",
       " 'repeal': 569,\n",
       " 'broken': 570,\n",
       " 'mother': 571,\n",
       " 'beat': 572,\n",
       " 'easy': 573,\n",
       " 'fired': 574,\n",
       " 'dear': 575,\n",
       " 'economic': 576,\n",
       " 'safe': 577,\n",
       " 'thing': 578,\n",
       " 'aide': 579,\n",
       " 'fish': 580,\n",
       " 'fashion': 581,\n",
       " 'nation': 582,\n",
       " 'contest': 583,\n",
       " 'push': 584,\n",
       " 'match': 585,\n",
       " 'watch': 586,\n",
       " 'womens': 587,\n",
       " 'turns': 588,\n",
       " 'target': 589,\n",
       " 'weight': 590,\n",
       " 'paid': 591,\n",
       " 'lets': 592,\n",
       " 'pregnancy': 593,\n",
       " 'warm': 594,\n",
       " 'hollywood': 595,\n",
       " 'any': 596,\n",
       " 'freedom': 597,\n",
       " 'cold': 598,\n",
       " 'spring': 599,\n",
       " 'success': 600,\n",
       " 'losing': 601,\n",
       " 'ever': 602,\n",
       " 'lose': 603,\n",
       " 'dr': 604,\n",
       " 'scandal': 605,\n",
       " 'homeland': 606,\n",
       " 'shooting': 607,\n",
       " '100': 608,\n",
       " 'abortion': 609,\n",
       " '9': 610,\n",
       " 'common': 611,\n",
       " 'teams': 612,\n",
       " 'drugs': 613,\n",
       " 'defense': 614,\n",
       " 'low': 615,\n",
       " 'fine': 616,\n",
       " 'courts': 617,\n",
       " 'finale': 618,\n",
       " 'human': 619,\n",
       " 'review': 620,\n",
       " 'leads': 621,\n",
       " 'yet': 622,\n",
       " 'water': 623,\n",
       " 'bring': 624,\n",
       " 'welcome': 625,\n",
       " 'blame': 626,\n",
       " 'between': 627,\n",
       " 'beyond': 628,\n",
       " 'both': 629,\n",
       " 'part': 630,\n",
       " 'russians': 631,\n",
       " 'killed': 632,\n",
       " 'happy': 633,\n",
       " 'oil': 634,\n",
       " 'simple': 635,\n",
       " 'spending': 636,\n",
       " 'ethics': 637,\n",
       " 'cities': 638,\n",
       " 'edge': 639,\n",
       " 'hall': 640,\n",
       " 'affair': 641,\n",
       " 'fans': 642,\n",
       " 'growth': 643,\n",
       " 'energy': 644,\n",
       " 'immigrants': 645,\n",
       " 'democratic': 646,\n",
       " 'manhattan': 647,\n",
       " 'grows': 648,\n",
       " 'longer': 649,\n",
       " 'chaos': 650,\n",
       " 'quiet': 651,\n",
       " 'killing': 652,\n",
       " 'hopes': 653,\n",
       " 'fears': 654,\n",
       " 'list': 655,\n",
       " 'different': 656,\n",
       " 'whos': 657,\n",
       " 'olympics': 658,\n",
       " 'key': 659,\n",
       " 'labor': 660,\n",
       " 'never': 661,\n",
       " 'ryan': 662,\n",
       " 'car': 663,\n",
       " 'medicine': 664,\n",
       " 'hit': 665,\n",
       " 'liberal': 666,\n",
       " 'blue': 667,\n",
       " 'fox': 668,\n",
       " 'raising': 669,\n",
       " 'return': 670,\n",
       " 'presidential': 671,\n",
       " 'center': 672,\n",
       " 'space': 673,\n",
       " 'study': 674,\n",
       " 'fargo': 675,\n",
       " 'ride': 676,\n",
       " 'theres': 677,\n",
       " 'lessons': 678,\n",
       " 'mean': 679,\n",
       " 'investors': 680,\n",
       " 'ends': 681,\n",
       " 'train': 682,\n",
       " 'wish': 683,\n",
       " 'birth': 684,\n",
       " 'paris': 685,\n",
       " 'sees': 686,\n",
       " 'charge': 687,\n",
       " 'turkey': 688,\n",
       " 'eye': 689,\n",
       " 'step': 690,\n",
       " 'system': 691,\n",
       " 'press': 692,\n",
       " 'millions': 693,\n",
       " 'government': 694,\n",
       " 'point': 695,\n",
       " '15': 696,\n",
       " 'limits': 697,\n",
       " 'special': 698,\n",
       " 'become': 699,\n",
       " 'turning': 700,\n",
       " 'short': 701,\n",
       " 'india': 702,\n",
       " 'chinas': 703,\n",
       " 'very': 704,\n",
       " 'sets': 705,\n",
       " 'four': 706,\n",
       " 'peace': 707,\n",
       " 'embrace': 708,\n",
       " 'line': 709,\n",
       " 'class': 710,\n",
       " 'decision': 711,\n",
       " 'familiar': 712,\n",
       " 'memo': 713,\n",
       " 'drag': 714,\n",
       " 'gone': 715,\n",
       " 'offer': 716,\n",
       " 'dreamers': 717,\n",
       " 'every': 718,\n",
       " 'speak': 719,\n",
       " 'light': 720,\n",
       " 'puts': 721,\n",
       " 'sick': 722,\n",
       " 'porn': 723,\n",
       " 'head': 724,\n",
       " 'valley': 725,\n",
       " 'ok': 726,\n",
       " 'few': 727,\n",
       " 'kind': 728,\n",
       " 'jail': 729,\n",
       " 'yorks': 730,\n",
       " 'aides': 731,\n",
       " 'premiere': 732,\n",
       " 'record': 733,\n",
       " 'older': 734,\n",
       " 'girl': 735,\n",
       " 'patients': 736,\n",
       " 'effect': 737,\n",
       " '13': 738,\n",
       " 'fury': 739,\n",
       " 'magic': 740,\n",
       " 'enough': 741,\n",
       " 'name': 742,\n",
       " 'add': 743,\n",
       " 'classic': 744,\n",
       " 'books': 745,\n",
       " 'phone': 746,\n",
       " 'promise': 747,\n",
       " 'jersey': 748,\n",
       " 'gives': 749,\n",
       " 'shift': 750,\n",
       " 'same': 751,\n",
       " 'secrets': 752,\n",
       " 'va': 753,\n",
       " 'stay': 754,\n",
       " 'amid': 755,\n",
       " 'thinks': 756,\n",
       " 'pet': 757,\n",
       " 'shot': 758,\n",
       " 'google': 759,\n",
       " 'everyone': 760,\n",
       " 'maybe': 761,\n",
       " 'deadly': 762,\n",
       " 'gave': 763,\n",
       " 'false': 764,\n",
       " 'walk': 765,\n",
       " 'always': 766,\n",
       " 'dog': 767,\n",
       " 'sea': 768,\n",
       " 'sleep': 769,\n",
       " '12': 770,\n",
       " 'hits': 771,\n",
       " 'believe': 772,\n",
       " 'taste': 773,\n",
       " 'kill': 774,\n",
       " 'deals': 775,\n",
       " 'kushner': 776,\n",
       " 'charges': 777,\n",
       " 'chance': 778,\n",
       " 'die': 779,\n",
       " 'trying': 780,\n",
       " 'course': 781,\n",
       " 'streets': 782,\n",
       " 'service': 783,\n",
       " 'alienist': 784,\n",
       " 'trouble': 785,\n",
       " 'robots': 786,\n",
       " 'rupauls': 787,\n",
       " 'gay': 788,\n",
       " 'super': 789,\n",
       " 'moves': 790,\n",
       " 'lot': 791,\n",
       " 'public': 792,\n",
       " 'protests': 793,\n",
       " 'noah': 794,\n",
       " 'fresh': 795,\n",
       " 'colbert': 796,\n",
       " 'guilty': 797,\n",
       " 'strikes': 798,\n",
       " 'iran': 799,\n",
       " 'toll': 800,\n",
       " 'fed': 801,\n",
       " 'storm': 802,\n",
       " 'details': 803,\n",
       " 'mexico': 804,\n",
       " 'teach': 805,\n",
       " 'silicon': 806,\n",
       " 'hours': 807,\n",
       " 'transgender': 808,\n",
       " 'online': 809,\n",
       " 'lies': 810,\n",
       " 'overlooked': 811,\n",
       " 'technology': 812,\n",
       " 'bright': 813,\n",
       " 'avoid': 814,\n",
       " 'calls': 815,\n",
       " 'chef': 816,\n",
       " 'industry': 817,\n",
       " 'reach': 818,\n",
       " 'votes': 819,\n",
       " 'pass': 820,\n",
       " 'diabetes': 821,\n",
       " 'trail': 822,\n",
       " 'crime': 823,\n",
       " 'disaster': 824,\n",
       " 'rate': 825,\n",
       " 'split': 826,\n",
       " 'southern': 827,\n",
       " 'federal': 828,\n",
       " 'wait': 829,\n",
       " 'lower': 830,\n",
       " 'finds': 831,\n",
       " 'person': 832,\n",
       " 'front': 833,\n",
       " 'text': 834,\n",
       " 'director': 835,\n",
       " 'which': 836,\n",
       " 'billion': 837,\n",
       " 'order': 838,\n",
       " 'intelligence': 839,\n",
       " 'equal': 840,\n",
       " 'calling': 841,\n",
       " 'used': 842,\n",
       " 'koreas': 843,\n",
       " 'colleges': 844,\n",
       " 'gap': 845,\n",
       " 'math': 846,\n",
       " 'clues': 847,\n",
       " 'took': 848,\n",
       " 'housing': 849,\n",
       " 'cover': 850,\n",
       " 'paul': 851,\n",
       " 'uber': 852,\n",
       " 'kitchen': 853,\n",
       " 'steps': 854,\n",
       " 'hell': 855,\n",
       " 'angry': 856,\n",
       " 'sweet': 857,\n",
       " 'box': 858,\n",
       " 'photos': 859,\n",
       " 'dirty': 860,\n",
       " 'arms': 861,\n",
       " 'nothing': 862,\n",
       " 'divide': 863,\n",
       " 'reading': 864,\n",
       " 'worse': 865,\n",
       " 'worst': 866,\n",
       " 'prison': 867,\n",
       " 'hero': 868,\n",
       " 'tune': 869,\n",
       " 'earth': 870,\n",
       " 'working': 871,\n",
       " 'sorry': 872,\n",
       " 'dogs': 873,\n",
       " 'canada': 874,\n",
       " 'artists': 875,\n",
       " 'jan': 876,\n",
       " 'using': 877,\n",
       " 'hands': 878,\n",
       " 'cash': 879,\n",
       " 'finally': 880,\n",
       " 'amazon': 881,\n",
       " 'harassment': 882,\n",
       " 'bowl': 883,\n",
       " 'soul': 884,\n",
       " 'assassination': 885,\n",
       " 'mayor': 886,\n",
       " 'clash': 887,\n",
       " 'exercise': 888,\n",
       " 'marijuana': 889,\n",
       " 'homeless': 890,\n",
       " 'hold': 891,\n",
       " 'trevor': 892,\n",
       " 'sound': 893,\n",
       " 'lie': 894,\n",
       " 'focus': 895,\n",
       " 'leaves': 896,\n",
       " 'foreign': 897,\n",
       " 'knows': 898,\n",
       " 'theyre': 899,\n",
       " 'youve': 900,\n",
       " 'tower': 901,\n",
       " 'strike': 902,\n",
       " 'returns': 903,\n",
       " 'games': 904,\n",
       " 'wonkish': 905,\n",
       " 'abroad': 906,\n",
       " 'heck': 907,\n",
       " 'legacy': 908,\n",
       " 'civil': 909,\n",
       " 'dying': 910,\n",
       " 'stress': 911,\n",
       " 'voting': 912,\n",
       " 'boss': 913,\n",
       " 'quiz': 914,\n",
       " 'seeking': 915,\n",
       " 'cost': 916,\n",
       " 'reporter': 917,\n",
       " 'cause': 918,\n",
       " 'full': 919,\n",
       " 'restaurant': 920,\n",
       " 'lines': 921,\n",
       " 'likely': 922,\n",
       " 'later': 923,\n",
       " 'clinton': 924,\n",
       " 'gender': 925,\n",
       " 'refugees': 926,\n",
       " 'driving': 927,\n",
       " 'senators': 928,\n",
       " 'empire': 929,\n",
       " 'station': 930,\n",
       " 'ivanka': 931,\n",
       " 'remember': 932,\n",
       " 'macron': 933,\n",
       " 'army': 934,\n",
       " 'secretary': 935,\n",
       " 'campaign': 936,\n",
       " 'coop': 937,\n",
       " 'ahead': 938,\n",
       " 'remain': 939,\n",
       " 'doing': 940,\n",
       " 'golden': 941,\n",
       " 'knew': 942,\n",
       " 'tiny': 943,\n",
       " 'reporting': 944,\n",
       " 'tries': 945,\n",
       " 'approach': 946,\n",
       " 'those': 947,\n",
       " 'read': 948,\n",
       " 'wild': 949,\n",
       " 'hear': 950,\n",
       " 'deep': 951,\n",
       " 'months': 952,\n",
       " 'since': 953,\n",
       " 'letter': 954,\n",
       " 'cooking': 955,\n",
       " 'shifts': 956,\n",
       " 'mothers': 957,\n",
       " 'lady': 958,\n",
       " 'boys': 959,\n",
       " 'signs': 960,\n",
       " 'flight': 961,\n",
       " 'madness': 962,\n",
       " 'toward': 963,\n",
       " 'major': 964,\n",
       " 'god': 965,\n",
       " 'terror': 966,\n",
       " 'blasio': 967,\n",
       " 'late': 968,\n",
       " 'stories': 969,\n",
       " 'led': 970,\n",
       " 'ideas': 971,\n",
       " 'winners': 972,\n",
       " 'falling': 973,\n",
       " 'internet': 974,\n",
       " 'missile': 975,\n",
       " 'came': 976,\n",
       " 'spy': 977,\n",
       " 'jeff': 978,\n",
       " 'running': 979,\n",
       " 'message': 980,\n",
       " 'vows': 981,\n",
       " 'assault': 982,\n",
       " 'issue': 983,\n",
       " 'ally': 984,\n",
       " 'murder': 985,\n",
       " 'park': 986,\n",
       " 'unlikely': 987,\n",
       " 'facing': 988,\n",
       " 'diet': 989,\n",
       " 'comments': 990,\n",
       " 'waiting': 991,\n",
       " 'opioids': 992,\n",
       " 'begins': 993,\n",
       " 'sanctions': 994,\n",
       " 'partisan': 995,\n",
       " 'families': 996,\n",
       " 'tree': 997,\n",
       " 'gold': 998,\n",
       " 'refugee': 999,\n",
       " 'nfl': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index # Ordered based on freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[46, 1601],\n",
       " [46, 1601, 1],\n",
       " [46, 1601, 1, 1951],\n",
       " [46, 1601, 1, 1951, 120],\n",
       " [122, 331],\n",
       " [122, 331, 1952],\n",
       " [122, 331, 1952, 2],\n",
       " [122, 331, 1952, 2, 125],\n",
       " [122, 331, 1952, 2, 125, 2484],\n",
       " [122, 331, 1952, 2, 125, 2484, 812]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sequences = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10] # Each document has multiple rows. 1-2, 1-2-3, 1-2-3-4 words (n-grams)\n",
    "# Teaching the model to account to longer distances. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to *pad* our input sequences to make them all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences) \n",
    "# All inputs need to be same lenght. \n",
    "# adding zeros to the start of shorted sequences \n",
    "# predictors = input vectors \n",
    "# labels = words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len # 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the ```create_model()``` function created above to initialize a model, telling the model the length of sequences and the total size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 10:30:28.139544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 10)            112650    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11265)             1137765   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,294,815\n",
      "Trainable params: 1,294,815\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is exactly the same as last week, but instead of document labels, we're fitting the model to predict next word.\n",
    "\n",
    "*NB!* This will take some time to train! It took me 35 minutes on UCloud 32xCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "405/405 [==============================] - 32s 78ms/step - loss: 7.0692\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 31s 78ms/step - loss: 6.9384\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 30s 75ms/step - loss: 6.8037\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 6.6624\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 6.5273\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 27s 68ms/step - loss: 6.3887\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 27s 66ms/step - loss: 6.2567\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 6.1338\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 6.0093\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 27s 68ms/step - loss: 5.8919\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 5.7781\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 5.6676\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 5.5575\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 5.4522\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 5.3490\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 5.2503\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 5.1561\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 27s 68ms/step - loss: 5.0594\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 4.9702\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 28s 70ms/step - loss: 4.8838\n",
      "Epoch 21/100\n",
      "405/405 [==============================] - 31s 77ms/step - loss: 4.8003\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 31s 77ms/step - loss: 4.7151\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 32s 78ms/step - loss: 4.6386\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 31s 76ms/step - loss: 4.5647\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 32s 78ms/step - loss: 4.4941\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 31s 77ms/step - loss: 4.4258\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 31s 77ms/step - loss: 4.3614\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 30s 75ms/step - loss: 4.2922\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 30s 75ms/step - loss: 4.2381\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 29s 73ms/step - loss: 4.1799\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 28s 70ms/step - loss: 4.1217\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 4.0719\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 4.0182\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 3.9715\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 3.9247\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 3.8766\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 28s 68ms/step - loss: 3.8347\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 27s 68ms/step - loss: 3.7921\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 31s 76ms/step - loss: 3.7457\n",
      "Epoch 40/100\n",
      " 13/405 [..............................] - ETA: 29s - loss: 3.6229"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(predictors, \n\u001b[1;32m      2\u001b[0m                     label, \n\u001b[1;32m      3\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, # Updates weights after 128 \n",
    "                    verbose=1)\n",
    "\n",
    "# In notebooks, a models history is saved. So if the model has run one time with 100 epoch and you start it again it will run for 200 intotal.\n",
    "# You either need to create the model again ( Above chunck) or use tensor flow functiion clear history."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has trained, we can then use this to generate *new text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Nerual Networks Says A Columnist Of A Backup Quarterback Rape Case Dies\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Nerual Networks\", 10, model, max_sequence_len)) # word you want, words to come after, model, make the sequence 24 in total."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the embedding layer as a trainable parameter, we can instead using a *pretrained word embedding* model like ```word2vec```.\n",
    "\n",
    "In the following examples, we're using [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). These are trained a little differently from ```word2vec``` but they behave in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\"path/to/glove/vectors\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some variables that we're going to use later.\n",
    "\n",
    "With hits and misses, we're counting how many words in the corpus vocabulary have a corresponding GloVe embedding; misses are the words which appear in our vocabulary but which do not have a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer - notice that this is different\n",
    "    model.add(Embedding(\n",
    "            total_words,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=input_len)\n",
    "    )\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"china\", 30, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
